# =======================================================================
# Platform Name            kafka-workshop
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  1.15.0-preview
# =======================================================================
version: '3.5'
networks:
  default:
    name: kafka-workshop
# enforce some dependencies
# backward compatiblity to platform < 1.14.0
# enforce some dependencies
# enforce some dependencies
# enforce some dependencies
# Enable PostgreSQL or MySQL for MLflow server
services:
  #  ================================== Zookeeper ========================================== #
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.1.0
    container_name: zookeeper-1
    hostname: zookeeper-1
    labels:
      com.platys.name: zookeeper
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
#  ================================== Kafka ========================================== #
  kafka-1:
    image: confluentinc/cp-kafka:7.1.0
    container_name: kafka-1
    hostname: kafka-1
    labels:
      com.platys.name: kafka
    depends_on:
      - zookeeper-1
    ports:
      - 9092:9092
      - 19092:19092
      - 29092:29092
      - 39092:39092
      - 9992:9992
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-1:19092,LISTENER_LOCAL://kafka-1:39092,LISTENER_DOCKERHOST://kafka-1:29092,LISTENER_EXTERNAL://kafka-1:9092
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-1:19092,LISTENER_LOCAL://localhost:39092,LISTENER_DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29092,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  kafka-2:
    image: confluentinc/cp-kafka:7.1.0
    container_name: kafka-2
    hostname: kafka-2
    labels:
      com.platys.name: kafka
    depends_on:
      - zookeeper-1
    ports:
      - 9093:9093
      - 19093:19093
      - 29093:29093
      - 39093:39093
      - 9993:9993
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-2:19093,LISTENER_LOCAL://kafka-2:39093,LISTENER_DOCKERHOST://kafka-2:29093,LISTENER_EXTERNAL://kafka-2:9093
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-2:19093,LISTENER_LOCAL://localhost:39093,LISTENER_DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29093,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  kafka-3:
    image: confluentinc/cp-kafka:7.1.0
    container_name: kafka-3
    hostname: kafka-3
    labels:
      com.platys.name: kafka
    depends_on:
      - zookeeper-1
    ports:
      - 9094:9094
      - 19094:19094
      - 29094:29094
      - 39094:39094
      - 9994:9994
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT,LISTENER_DOCKERHOST:PLAINTEXT,LISTENER_EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: LISTENER_INTERNAL://kafka-3:19094,LISTENER_LOCAL://kafka-3:39094,LISTENER_DOCKERHOST://kafka-3:29094,LISTENER_EXTERNAL://kafka-3:9094
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-3:19094,LISTENER_LOCAL://localhost:39094,LISTENER_DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29094,LISTENER_EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Kafka Connect ========================================== #
  kafka-connect-1:
    image: confluentinc/cp-kafka-connect:7.1.0
    container_name: kafka-connect-1
    hostname: kafka-connect-1
    labels:
      com.platys.name: kafka-connect
      com.platys.restapi.title: Kafka Connect REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8083
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      CONNECT_LISTENERS: http://0.0.0.0:8083
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-1
      CONNECT_REST_ADVERTISED_PORT: 8083
      CONNECT_GROUP_ID: kafka-connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/addl-plugins,/etc/kafka-connect/cflthub-plugins
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All
      #CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      # External secrets config
      # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: file
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: org.apache.kafka.common.config.provider.FileConfigProvider
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/opentelemetry/agents:/otel
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        echo "Installing Connectors"
        mkdir -p /etc/kafka-connect/cflthub-plugins
        for i in $$(echo "confluentinc/kafka-connect-jdbc:10.0.0,confluentinc/kafka-connect-mqtt:1.3.0,debezium/debezium-connector-postgresql:1.8.1" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/cflthub-plugins --verbose "$$i"
        done
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity
    restart: unless-stopped
  kafka-connect-2:
    image: confluentinc/cp-kafka-connect:7.1.0
    container_name: kafka-connect-2
    hostname: kafka-connect-2
    labels:
      com.platys.name: kafka-connect
      com.platys.restapi.title: Kafka Connect REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8084
    ports:
      - 8084:8084
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      CONNECT_LISTENERS: http://0.0.0.0:8084
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect-2
      CONNECT_REST_ADVERTISED_PORT: 8084
      CONNECT_GROUP_ID: kafka-connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: '[%d] %p %X{connector.context}%m (%c:%L)%n'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/addl-plugins,/etc/kafka-connect/cflthub-plugins
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All
      #CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-4.0.0.jar
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      # External secrets config
      # See https://docs.confluent.io/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: file
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: org.apache.kafka.common.config.provider.FileConfigProvider
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/opentelemetry/agents:/otel
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        echo "Installing Connectors"
        mkdir -p /etc/kafka-connect/cflthub-plugins
        for i in $$(echo "confluentinc/kafka-connect-jdbc:10.0.0,confluentinc/kafka-connect-mqtt:1.3.0,debezium/debezium-connector-postgresql:1.8.1" | sed "s/,/ /g")
        do
          confluent-hub install --no-prompt --component-dir /etc/kafka-connect/cflthub-plugins --verbose "$$i"
        done
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep infinity      
    restart: unless-stopped
  #  ================================== ksqlDB ========================================== #
  ksqldb-server-1:
    image: confluentinc/ksqldb-server:0.26.0
    hostname: ksqldb-server-1
    container_name: ksqldb-server-1
    labels:
      com.platys.name: ksqldb
      com.platys.restapi.title: ksqlDB Server REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8088
    ports:
      - 8088:8088
      - 1095:1095
    environment:
      #KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: 'ksql_processing_log'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_APPLICATION_ID: ksqldb-cluster
      KSQL_KSQL_SERVICE_ID: ksqldb-cluster
      KSQL_HOST_NAME: ksqldb-server-1
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: ''
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-1:8083,http://kafka-connect-2:8084
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: 1
      KSQL_KSQL_SINK_REPLICAS: 1
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 1
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_KSQL_HIDDEN_TOPICS: ^_.*,default_ksql_processing_log
      KSQL_KSQL_SUPPRESS_ENABLED: 'False'
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: '-1'
      KSQL_KSQL_QUERY_PULL_TABLE_SCAN_ENABLED: 'False'
      KSQL_CONFIG_DIR: /etc/ksqldb
      KSQL_KSQL_EXTENSION_DIR: /etc/ksqldb/ext/
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksqldb:/etc/ksqldb/ext
      - ./conf/ksqldb/etc/log4j.properties:/etc/ksqldb/log4j.properties
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
  ksqldb-server-2:
    image: confluentinc/ksqldb-server:0.26.0
    hostname: ksqldb-server-2
    container_name: ksqldb-server-2
    labels:
      com.platys.name: ksqldb
      com.platys.restapi.title: ksqlDB Server REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8089
    ports:
      - 8089:8088
      - 1096:1095
    environment:
      #KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: 'ksql_processing_log'
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'
      # For Demo purposes: improve resource utilization and avoid timeouts
      KSQL_KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_PRODUCER_ENABLE_IDEMPOTENCE: 'true'
      KSQL_APPLICATION_ID: ksqldb-cluster
      KSQL_KSQL_SERVICE_ID: ksqldb-cluster
      KSQL_HOST_NAME: ksqldb-server-2
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_RESPONSE_HTTP_HEADERS_CONFIG: ''
      KSQL_KSQL_CONNECT_URL: http://kafka-connect-1:8083,http://kafka-connect-2:8084
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KSQL_KSQL_INTERNAL_TOPIC_REPLICAS: 1
      KSQL_KSQL_SINK_REPLICAS: 1
      KSQL_KSQL_STREAMS_REPLICATION_FACTOR: 1
      KSQL_KSQL_QUERY_PULL_METRICS_ENABLED: 'true'
      KSQL_KSQL_HIDDEN_TOPICS: ^_.*,default_ksql_processing_log
      KSQL_KSQL_SUPPRESS_ENABLED: 'False'
      KSQL_KSQL_SUPPRESS_BUFFER_SIZE_BYTES: '-1'
      KSQL_KSQL_QUERY_PULL_TABLE_SCAN_ENABLED: 'False'
      KSQL_CONFIG_DIR: /etc/ksqldb
      KSQL_KSQL_EXTENSION_DIR: /etc/ksqldb/ext/
      KSQL_JMX_OPTS: -Djava.rmi.server.hostname=localhost -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1095 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.rmi.port=1095
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/ksqldb:/etc/ksqldb/ext
      - ./conf/ksqldb/etc/log4j.properties:/etc/ksqldb/log4j.properties
      - ./plugins/kafka-connect/connectors:/etc/kafka-connect/addl-plugins
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
  # Access the cli by running:
  # > docker exec -it ksqldb-cli ksql http://ksqldb-server-1:8088
  ksqldb-cli:
    image: confluentinc/ksqldb-cli:0.26.0
    container_name: ksqldb-cli
    hostname: ksqldb-cli
    labels:
      com.platys.name: ksqldb-cli
    depends_on:
      - ksqldb-server-1
    volumes:
      - ./data-transfer:/data-transfer
    entrypoint: /bin/sh
    tty: true
    restart: unless-stopped
  #  ================================== Confluent REST Proxy ========================================== #
  kafka-rest-1:
    image: confluentinc/cp-kafka-rest:7.1.0
    container_name: kafka-rest-1
    hostname: kafka-rest-1
    labels:
      com.platys.name: kafka-rest
      com.platys.restapi.title: Kafka REST Proxy
      com.platys.restapi.url: http://${PUBLIC_IP}:18086
    ports:
      - 18086:8086
    environment:
      KAFKA_REST_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      KAFKA_REST_LISTENERS: http://0.0.0.0:8086
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KAFKA_REST_HOST_NAME: kafka-rest-1
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Confluent Schema Registry ========================================== #
  schema-registry-1:
    image: confluentinc/cp-schema-registry:7.1.0
    hostname: schema-registry-1
    container_name: schema-registry-1
    labels:
      com.platys.name: schema-registry
      com.platys.restapi.title: Schema Registry REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:8081
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: PLAINTEXT
      SCHEMA_REGISTRY_GROUP_ID: schema-registry
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: 'True'
      SCHEMA_REGISTRY_MODE_MUTABILITY: 'True'
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: backward
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: info
      SCHEMA_REGISTRY_DEBUG: 'False'
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
  #  ================================== Schema Registry UI ========================================== #
  schema-registry-ui:
    image: landoop/schema-registry-ui:latest
    container_name: schema-registry-ui
    hostname: schema-registry-ui
    labels:
      com.platys.name: schema-registry-ui
      com.platys.webui.title: Confluent Schema Registry UI
      com.platys.webui.url: http://${PUBLIC_IP}:28102
    ports:
      - 28102:8000
    environment:
      SCHEMAREGISTRY_URL: http://${PUBLIC_IP}:8081
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/resolv.conf:/etc/resolv.conf:ro
    restart: unless-stopped
  #  ================================== Kafka Topics UI ========================================== #
  kafka-topics-ui:
    image: landoop/kafka-topics-ui:latest
    container_name: kafka-topics-ui
    hostname: kafka-topics-ui
    labels:
      com.platys.name: kafka-topics-ui
      com.platys.webui.title: Kafka Topics UI
      com.platys.webui.url: http://${PUBLIC_IP}:28141
    ports:
      - 28141:8000
    environment:
      KAFKA_REST_PROXY_URL: http://kafka-rest-1:8086
      LAZY_LOAD_TOPIC_META: 'false'
      PROXY: 'true'
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/resolv.conf:/etc/resolv.conf:ro
    restart: unless-stopped
  #  ================================== Kafka Connect UI ========================================== #
  kafka-connect-ui:
    image: landoop/kafka-connect-ui:latest
    container_name: kafka-connect-ui
    hostname: kafka-connect-ui
    labels:
      com.platys.name: kafka-connect-ui
      com.platys.webui.title: Kafka Connect UI
      com.platys.webui.url: http://${PUBLIC_IP}:28103
    ports:
      - 28103:8000
    environment:
      CONNECT_URL: http://kafka-connect-1:8083,http://kafka-connect-2:8084
      PROXY: 'true'
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/resolv.conf:/etc/resolv.conf:ro
    restart: unless-stopped
  #  ================================== Cluster Manager for Apache Kafka (CMAK) ========================================== #
  cmak:
    image: trivadis/cmak:latest
    container_name: cmak
    hostname: cmak
    labels:
      com.platys.name: cmak
      com.platys.webui.title: Cluster Manager for Apache Kafka UI
      com.platys.webui.url: http://${PUBLIC_IP}:28104
    ports:
      - 28104:9000
    environment:
      ZK_HOSTS: zookeeper-1:2181
      APPLICATION_SECRET: abc123!
      KAFKA_MANAGER_AUTH_ENABLED: 'false'
      KAFKA_MANAGER_USERNAME: admin
      KAFKA_MANAGER_PASSWORD: abc123!
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Apache Kafka HQ (AKHQ) ========================================== #
  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    hostname: akhq
    labels:
      com.platys.name: akhq
      com.platys.webui.title: Apache Kafka AQ UI
      com.platys.webui.url: http://${PUBLIC_IP}:28107
    ports:
      - 28107:8080
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: 'kafka-1:19092,kafka-2:19093,kafka-3:19094'
              schema-registry:
                url: "http://schema-registry-1:8081"
                type: "confluent"
              connect:
                - name: "connect-1"
                  url: "http://kafka-connect-1:8083"
                - name: "connect-2"
                  url: "http://kafka-connect-2:8084"
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== StreamSets DataCollector ========================================== #
  streamsets-1:
    image: streamsets/datacollector:3.22.2
    container_name: streamsets-1
    hostname: streamsets-1
    labels:
      com.platys.name: streamsets
      com.platys.webui.title: StreamSets Data Collector UI
      com.platys.webui.url: http://${PUBLIC_IP}:18630
      com.platys.restapi.title: StreamSets Data Collector REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:18630/collector/restapi
    ports:
      - 18630:18630
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
      SDC_INSTALL_STAGES: ''
      SDC_INSTALL_ENTERPRISE_STAGES: ''
      SDC_JAVA_OPTS: -Xmx2g -Xms2g -Dlog4j2.formatMsgNoLookups=true
      SDC_JAVA8_OPTS: -XX:+UseG1GC
      SDC_CONF_MONITOR_MEMORY: 'true'
      SDC_CONF_PIPELINE_MAX_RUNNERS_COUNT: 50
      SDC_CONF_http_authentication: form
      SDC_CONF_RUNTIME_CONF_LOCATION: embedded
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/streamsets/pre-docker-entrypoint.sh:/pre-docker-entrypoint.sh
      - ./plugins/streamsets/user-libs:/opt/streamsets-datacollector-user-libs:Z
      - ./plugins/streamsets/libs-extras/streamsets-datacollector-jdbc-lib/:/opt/streamsets-datacollector-3.22.2/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/:Z
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    user: '1000'
    command:
      - dc
      - -exec
      - -verbose
    entrypoint:
      - /pre-docker-entrypoint.sh
    restart: unless-stopped
  #  ================================== NiFi ========================================== #
  nifi-1:
    image: apache/nifi:1.16.1
    container_name: nifi-1
    hostname: nifi-1
    labels:
      com.platys.name: nifi
      com.platys.webui.title: Apache NiFi UI Node 18080
      com.platys.webui.url: https://${PUBLIC_IP}:18080/nifi
    ports:
      # HTTP
      - 18080:18080
      # Remote Input Socket
      - 10005:10005/tcp
    environment:
      NIFI_WEB_HTTPS_PORT: '18080'
      NIFI_WEB_HTTPS_HOST: nifi-1
      NIFI_WEB_PROXY_HOST: ${PUBLIC_IP}:18080,${DOCKER_HOST_IP}18080
      AUTH: tls
      KEYSTORE_PATH: /opt/certs/keystore.jks
      KEYSTORE_TYPE: JKS
      KEYSTORE_PASSWORD: PTz7kFl1rzX4wUtcDlurwV6gjm7vID9Ibgbe71N355w
      TRUSTSTORE_PATH: /opt/certs/truststore.jks
      TRUSTSTORE_TYPE: JKS
      TRUSTSTORE_PASSWORD: h8I20cJyr50rFwzJRZkBcodLM8ifnDqQK2kORds8TLk
      NIFI_SECURITY_USER_AUTHORIZER: single-user-authorizer
      NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER: single-user-provider
      NIFI_REMOTE_INPUT_SOCKET_PORT: '10005'
      NIFI_REMOTE_INPUT_HOST: nifi-1
      NIFI_SENSITIVE_PROPS_KEY: 12345678901234567890A
      SINGLE_USER_CREDENTIALS_USERNAME: nifi
      SINGLE_USER_CREDENTIALS_PASSWORD: 1234567890ACD
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/nifi/keystore.jks:/opt/certs/keystore.jks
      - ./conf/nifi/truststore.jks:/opt/certs/truststore.jks
    restart: unless-stopped
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: trivadis/apache-zeppelin:0.10.0-spark2.4.8-hadoop2.8
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.name: zeppelin
      com.platys.webui.title: Apache Zeppelin UI
      com.platys.webui.url: http://${PUBLIC_IP}:28080
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      SPARK_HADOOP_FS_S3A_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars:
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: in-memory
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_MEM: -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
      ZEPPELIN_ADMIN_USERNAME: admin
      ZEPPELIN_ADMIN_PASSWORD: changeme
      ZEPPELIN_USER_USERNAME: zeppelin
      ZEPPELIN_USER_PASSWORD: changeme
      # set spark-master for Zeppelin interpreter
      ZEPPELIN_SPARK_MASTER: spark://spark-master:7077
      ZEPPELIN_NOTEBOOK_DIR: notebook
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: 'True'
      PYSPARK_PYTHON: python3
      SPARK_SUBMIT_OPTIONS: ' --conf spark.ui.port=4050 --conf spark.driver.host=zeppelin --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4'
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/s3cfg:/root/.s3cfg.template
    restart: unless-stopped
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: jupyter/all-spark-notebook:spark-3.1.1
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: jupyter
      com.platys.webui.title: Jupyter UI
      com.platys.webui.url: http://${PUBLIC_IP}:28888
    ports:
      - 28888:8888
    environment:
      JUPYTER_ENABLE_LAB: 'true'
      JUPYTER_TOKEN: abc123!
      GRANT_SUDO: 'true'
      TINI_SUBREAPER: 'true'
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      SPARK_HADOOP_FS_S3A_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/jupyter/aws-java-sdk-bundle-1.11.375.jar:/usr/local/spark/jars/aws-java-sdk-bundle-1.11.375.jar
      - ./plugins/jupyter/hadoop-aws-3.2.1.jar:/usr/local/spark/jars/hadoop-aws-3.2.1.jar
      - ./plugins/jupyter/guava-27.1-jre.jar:/usr/local/spark/jars/guava-14.0.1.jar
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #
  postgresql:
    image: postgres:13
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: postgresql
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_MULTIPLE_DATABASES=demodb
      - POSTGRES_MULTIPLE_USERS=demo
      - POSTGRES_MULTIPLE_PASSWORDS=abc123!
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA=demo
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    command:
      - postgres
      - -c
      - wal_level=logical    
    restart: unless-stopped
  #  ================================== Adminer ========================================== #
  adminer:
    image: adminer:latest
    container_name: adminer
    hostname: adminer
    labels:
      com.platys.name: adminer
      com.platys.webui.title: Adminer UI
      com.platys.webui.url: http://${PUBLIC_IP}:28131
    ports:
      - 28131:8080
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2022-05-08T23-50-31Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://${PUBLIC_IP}:9000
    ports:
      - 9000:9000
      - 9010:9010
    environment:
      MINIO_ROOT_USER: V42FCGRVMK24JJ8DHUYG
      MINIO_ROOT_PASSWORD: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      MINIO_REGION_NAME: us-east-1
      MINIO_SERVER_URL: http://${PUBLIC_IP}
      #MINIO_PUBLIC_IPS: minio-1
      #MINIO_DOMAIN: minio.io
      #MINIO_DEFAULT_BUCKETS: 'admin-bucket,'
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data --console-address ":9010"
    restart: unless-stopped
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/minio/config.json:/root/.mc/config.json
    entrypoint:
      - /bin/sh
      - -c
      - |
        sleep 10
        mc alias set minio-1 http://minio-1:9000 V42FCGRVMK24JJ8DHUYG bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
        mc mb --ignore-existing minio-1/admin-bucket
           for i in $$(echo "" | sed "s/,/ /g")
        do
          mc mb --ignore-existing minio-1/$$i
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== Mosquitto ========================================== #
  mosquitto-1:
    image: eclipse-mosquitto:2.0
    hostname: mosquitto-1
    container_name: mosquitto-1
    labels:
      com.platys.name: mosquitto
    ports:
      - 1883:1883
      - 9101:9001
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf
    restart: unless-stopped
  #  ================================== MQTT UI ========================================== #
  mqtt-ui:
    image: vergissberlin/hivemq-mqtt-web-client:latest
    container_name: mqtt-ui
    hostname: mqtt-ui
    labels:
      com.platys.name: hivemq-ui
      com.platys.webui.title: HiveMQ UI
      com.platys.webui.url: http://${PUBLIC_IP}:28136
    ports:
      - 28136:80
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Wetty ========================================== #
  wetty:
    image: svenihoney/wetty:latest
    container_name: wetty
    hostname: wetty
    labels:
      com.platys.name: wetty
      com.platys.webui.title: WeTTY UI
      com.platys.webui.url: http://${PUBLIC_IP}:3001
    ports:
      - 3001:3000
    environment:
      - REMOTE_SSH_SERVER=${DOCKER_HOST_IP}
      - REMOTE_SSH_PORT=22
      - REMOTE_SSH_USER=
      - WETTY_PORT=3000
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: trivadis/markdown-web:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://${PUBLIC_IP}:80
    ports:
      - 80:80
    volumes:
      - ./artefacts:/home/python/markdown
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: kafka-workshop
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: 1.15.0-preview
      PLATYS_COPY_COOKBOOK_DATA: 'True'
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
volumes:
  data-transfer-vol:
    name: data_transfer_vol
